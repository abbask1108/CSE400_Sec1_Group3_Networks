\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\documentclass[12pt]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{hyperref}

\onehalfspacing

\begin{document}

\begin{center}
{\Large \textbf{CSE400 – Fundamentals of Probability in Computing}}\\[4pt]
{\large \textbf{Lecture 5: Bayes’ Theorem, Random Variables, and Probability Mass Function}}\\[8pt]

\textbf{Name:} Chaudhari Ansh Nareshkumar\\
\textbf{Email ID:} ansh.c1@ahduni.edu.in\\[6pt]

Ahmedabad University\\
January 20, 2026
\end{center}

\vspace{1em}

\section{Bayes’ Theorem}

\subsection{Weighted Average of Conditional Probabilities}

Let $A$ and $B$ be events.  
Event $A$ can be expressed as:
\[
A = AB \cup AB^c
\]
because for an outcome to be in $A$, it must either be in both $A$ and $B$, or in $A$ and not in $B$.

The events $AB$ and $AB^c$ are mutually exclusive. Hence, by Axiom 3:
\[
\Pr(A) = \Pr(AB) + \Pr(AB^c)
\]

Using the definition of conditional probability:
\[
\Pr(AB) = \Pr(A \mid B)\Pr(B)
\]
\[
\Pr(AB^c) = \Pr(A \mid B^c)\Pr(B^c)
\]

Thus,
\[
\Pr(A) = \Pr(A \mid B)\Pr(B) + \Pr(A \mid B^c)[1 - \Pr(B)]
\]

\textbf{Conclusion:}  
The probability of event $A$ is a \emph{weighted average} of the conditional probabilities, where the weights are the probabilities of the conditioning events.

\subsection{Learning by Example: Example 3.1 (Part 1)}

An insurance company classifies people as:
\begin{itemize}
\item Accident prone
\item Not accident prone
\end{itemize}

Given:
\[
\Pr(\text{Accident} \mid \text{Accident prone}) = 0.4
\]
\[
\Pr(\text{Accident} \mid \text{Not accident prone}) = 0.2
\]
\[
\Pr(\text{Accident prone}) = 0.3
\]

Let:
\[
A_1 = \text{event that policyholder has an accident within a year}
\]
\[
A = \text{event that policyholder is accident prone}
\]

Using conditioning:
\[
\Pr(A_1) = \Pr(A_1 \mid A)\Pr(A) + \Pr(A_1 \mid A^c)\Pr(A^c)
\]

Substituting values:
\[
\Pr(A_1) = (0.4)(0.3) + (0.2)(0.7) = 0.26
\]

\subsection{Learning by Example: Example 3.1 (Part 2)}

Given that a policyholder has an accident, find the probability that they are accident prone.

We compute:
\[
\Pr(A \mid A_1) = \frac{\Pr(A A_1)}{\Pr(A_1)}
\]

Using:
\[
\Pr(A A_1) = \Pr(A)\Pr(A_1 \mid A)
\]

Thus,
\[
\Pr(A \mid A_1) = \frac{(0.3)(0.4)}{0.26} = \frac{6}{13}
\]

\section{Formal Introduction}

\subsection{Law of Total Probability}

Suppose $B_1, B_2, \dots, B_n$ are mutually exclusive events such that:
\[
\bigcup_{i=1}^{n} B_i = B
\]

Since exactly one of the events must occur, we write:
\[
A = \bigcup_{i=1}^{n} AB_i
\]

Using mutual exclusivity:
\[
\Pr(A) = \sum_{i=1}^{n} \Pr(AB_i)
\]

Applying conditional probability:
\[
\Pr(A) = \sum_{i=1}^{n} \Pr(A \mid B_i)\Pr(B_i)
\]

This is known as the \textbf{Law of Total Probability}.

\subsection{Bayes Formula}

Using:
\[
\Pr(AB_i) = \Pr(B_i \mid A)\Pr(A)
\]

We obtain:
\[
\Pr(B_i \mid A) = \frac{\Pr(A \mid B_i)\Pr(B_i)}{\sum_{j=1}^{n} \Pr(A \mid B_j)\Pr(B_j)}
\]

This is known as the \textbf{Bayes Formula (Proposition 3.1)}.

Here:
\begin{itemize}
\item $\Pr(B_i)$ is the \textbf{a priori probability}
\item $\Pr(B_i \mid A)$ is the \textbf{posteriori probability}
\end{itemize}

\section{Bayes Formula: Example 3.2}

Three cards:
\begin{itemize}
\item RR (both sides red)
\item BB (both sides black)
\item RB (one red, one black)
\end{itemize}

Let:
\[
R = \text{event that upturned side is red}
\]

We want:
\[
\Pr(RB \mid R)
\]

Using Bayes formula:
\[
\Pr(RB \mid R) = \frac{\Pr(R \mid RB)\Pr(RB)}{\Pr(R)}
\]

Where:
\[
\Pr(R) = \Pr(R \mid RR)\Pr(RR) + \Pr(R \mid RB)\Pr(RB) + \Pr(R \mid BB)\Pr(BB)
\]

Substituting values:
\[
\Pr(RB \mid R) = \frac{(1/2)(1/3)}{(1)(1/3) + (1/2)(1/3) + (0)(1/3)} = \frac{1}{3}
\]

\section{Random Variables}

\subsection{Definition}

A \textbf{random variable} is a real-valued function defined on a sample space $\Omega$:
\[
X: \Omega \rightarrow \mathbb{R}
\]

It assigns a real number $X(\omega)$ to each outcome $\omega \in \Omega$.

In this lecture, we restrict attention to \textbf{discrete random variables}.

\subsection{Distribution of a Random Variable}

Let $a$ be a value in the range of $X$.  
The event:
\[
\{ \omega \in \Omega : X(\omega) = a \}
\]
is written as $X = a$.

The probability:
\[
\Pr(X = a)
\]
defines the distribution of $X$.

\subsection{Example: Tossing 3 Fair Coins}

Let $Y$ be the number of heads.

\[
\Pr(Y=0)=\frac{1}{8}, \quad
\Pr(Y=1)=\frac{3}{8}, \quad
\Pr(Y=2)=\frac{3}{8}, \quad
\Pr(Y=3)=\frac{1}{8}
\]

Since $Y$ must take one of these values:
\[
\sum_{i=0}^{3} \Pr(Y=i) = 1
\]

\section{Probability Mass Function (PMF)}

\subsection{Definition}

A random variable that takes at most a countable number of values is called \textbf{discrete}.

Let $X$ be a discrete random variable with range:
\[
R_X = \{x_1, x_2, x_3, \dots\}
\]

The function:
\[
p_X(x) = \Pr(X = x)
\]
is called the \textbf{Probability Mass Function (PMF)} of $X$.

Since $X$ must take one of its values:
\[
\sum_x p_X(x) = 1
\]

\subsection{PMF Example: Two Independent Coin Tosses}

Let $X$ be the number of heads.

\[
p_X(x) =
\begin{cases}
\frac{1}{4}, & x=0 \text{ or } x=2 \\
\frac{1}{2}, & x=1 \\
0, & \text{otherwise}
\end{cases}
\]

\subsection{PMF Example}

Given:
\[
p(i) = c \frac{\lambda^i}{i!}, \quad i = 0,1,2,\dots
\]

Since:
\[
\sum_{i=0}^{\infty} p(i) = 1
\]

We obtain:
\[
c \sum_{i=0}^{\infty} \frac{\lambda^i}{i!} = 1
\]

Using:
\[
e^{\lambda} = \sum_{i=0}^{\infty} \frac{\lambda^i}{i!}
\]

Thus:
\[
c = e^{-\lambda}
\]

Hence:
\[
\Pr(X=0) = e^{-\lambda}
\]
\[
\Pr(X>2) = 1 - \sum_{i=0}^{2} e^{-\lambda}\frac{\lambda^i}{i!}
\]

\section*{End of Lecture 5}

\end{document}


\end{document}